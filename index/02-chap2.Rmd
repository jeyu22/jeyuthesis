---
output:
  pdf_document: default
  html_document: default
---
# Estimation and Inference in Linear Mixed Models {#rmd-basics}
<!--
This file is for including Chapter 2.  

Notice that it's also good practice to label your chapters and sections.  This will help you debug potential issues as you knit and allows you to link references throughout the document. Look for the reference to this chapter at the beginning of Chapter 3.

If labels aren't specified, they will automatically be generated from the header by changing the spaces to hyphens and capital letters to lowercase.  
-->
```{r load_packages2, include = FALSE}
library(mosaic)
library(kableExtra)
library(ggplot2)
library(SimMultiCorrData)
library(here)
library(tidyverse)
library(kableExtra)
knitr::opts_chunk$set(echo = FALSE, message = F, warning = F)
```

In chapter 1, we outlined the basics of analyzing longitudinal data and introduced linear mixed models. Next, we will examine inference of linear mixed models, and how methods such as Kenward-Roger (KR) and Satterthwaite can be used in situations where standard procedures for inference may produce questionable results. 

## Estimation
Regression coefficient values $\beta$ and the covariance matrix $\Sigma_i$ can be estimated using maximum likelihood estimation, which identifies values of $\beta$ and $\Sigma_i$ that best explain the observed data. These values are estimates that are denoted by $\hat\beta$ and $\hat\Sigma_i.$ When observations are independent of one another, maximizing the likelihood function for $\beta$ is equivalent to finding a value of $\hat\beta$ that minimizes the sum of the squares of the residuals. However, since there are repeated measurements of each individual that are not independent of one another we use the generalized least squares (GLS) estimator as shown by @fitzmaurice_applied_2011:

$$\hat\beta = \sum_{i=1}^N(X_i'\Sigma^{-1}_iX_i)^{-1}\sum_{i=1}^N(X_i'\Sigma^{-1}_iy_i).$$

The sampling distribution of $\hat\beta$ has mean $\beta$ and covariance:
$$\widehat{\text{Cov}}(\beta) = \sum_{i=1}^N(X_i'\Sigma^{-1}_iX_i)^{-1}.$$

The GLS estimator assumes that $\Sigma_i$ is known. However, since this isn't usually the case, we can substitute $\Sigma_i$ with a maximum likelihood estimate $\hat\Sigma_i.$ It can be shown that the properties of $\hat\beta$ still hold using an estimate of the covariance. These properties include $\hat\beta$ being a consistent estimator of $\beta$ and having sampling distribution that is multivariate normal with mean $\beta$ and covariance $\text{Cov}(\hat\beta) = \sum_{i=1}^{N}(X_i'\sum_i^{-1}X_i)^{-1}.$

While the maximum likelihood estimate of $\Sigma_i$ is adequate, a modified method known as restricted maximum likelihood (REML) estimation is suggested to reduce bias in finite samples [@fitzmaurice_applied_2011]. The bias originates from the fact that $\beta$ itself is also estimated from data, but is not accounted for when estimating covariance. In REML estimate of $\Sigma_i$, $\beta$ is removed from the likelihood function. This REML estimation of $\Sigma_i$ can be used in the GLS estimator for $\hat\beta$ mentioned above, and is recommended in place of the maximum likelihood estimator. 

One additional analysis that is possible with linear mixed effects models is predicting subject-specific responses. Given that $b_i$ is a random variable, we can predict it using:
$$E(b_i |Y_i) = GZ_i (\Sigma)^{-1}_i(Y_i-X_i\hat\beta).$$ Because the covariance of $Y_i$ is unknown, we can estimate both $G$ and $\Sigma_i$ using REML, creating $\hat b_i$, also known as the empirical BLUP [@fitzmaurice_applied_2011]. Thus, the equation for predicting the response profile is:
$$\hat Y_i = X_i\hat\beta +Z_i\hat b_i.$$

This equation to estimate the mean response profile can be extended to incorporate $R_i$, which represents within-subject variability. From this extension, we see that the equation and the empirical BLUP account for the weighting of both the within-subject variability and between-subject variability. If there is more within-subject variability, then more weight is assigned to $X_i\hat\beta$, the population mean response profile, in comparison to the subject's individual responses, and vice versa. 

## Inference 
In statistical inference, the goal is to make conclusions about the underlying characteristics of a set of data and establish a relationship between certain variables. Hypothesis testing is one of the primary examples of inference, and is carried out in order to assess the true value of a population parameter. In linear models, the significance of a slope parameter, $\beta_k$, is often assessed, where the null hypothesis, $H_0$ is $\beta_k = 0,$ and the alternative hypothesis $H_a$ is $\beta_k \neq 0.$ A test of the null hypothesis involves using a Wald statistic in the form $$Z = \frac{\hat\beta_k}{\sqrt{\widehat{Var}(\hat\beta_k)}},$$ which is then compared to the normal distribution, and a subsequent p-value is obtained. 

Building on foundations of a general linear hypothesis test, given a matrix $L$ of size $r \times p,$ where $r$ represents the number of estimable functions of $\beta,$ the Wald statistic  $$(L\hat\beta-L\beta)'[L(\widehat{Cov}(\hat\beta))L']^{-1}(L\hat\beta-L\beta)$$ is approximately $\chi^2(r)$ [@rencher_schaalje_2007]. For a null hypothesis $H_0: L\beta = 0,$ the test statistic is $$W^2 =(L\hat\beta)'[L(\widehat {Cov}(\hat\beta))L']^{-1}(L\hat\beta).$$ 

Likelihood ratio tests are another method to make inferences about $\beta$. While there are benefits to using the likelihood ratio test, the rest of this study will focus on method of using the Wald statistic.

### Inference in Small Sample Sizes 
One crucial assumption when conducting inference using the maximum likelihood estimate for $\beta$ is that the sample size is sufficient enough where it does not affect the estimate for $\Sigma_i.$ However, what happens when the sample size is too small? This causes $\hat\Sigma_i$ to underestimate the true variance, which in turn causes $\widehat {\text{Cov}}(\hat\beta)$ to be too small since it relies on the covariance estimator. If $\widehat {\text{Cov}}(\hat\beta)$ is too small, the denominator of the Wald test statistic is too small, so the test statistic is too big, leading to increased Type I error. One can see that the bias of the covariance estimator weakens the entire foundation of estimation and inference. 

There are limited cases where exact small-sample inference can be performed, but in most scenarios, an approximate small-sample method must be used, in which the statistic $$F = \frac{(L\hat\beta)'[L(\widehat {\text{Cov}}(\hat\beta))^-L']^{-1}(L\hat\beta)}{g}$$ follows a F distribution with numerator degrees of freedom $g,$ and unknown denominator degrees of freedom, DDF. There are several ways to approximate the DDF. 

Both Satterthwaite and KR are proposed methods of approximations to the DDF when conducting tests in order to account for the uncertainty of the covariance estimator. The KR method goes one step forward to also adjust the test statistic itself.

## Satterthwaite & Kenward Roger
@hrong-tai_fai_approximate_1996 propose using Satterthwaite approximation of DF for the aforementioned F statistic when the hypothesis of interest is $H_0 : L\beta = 0,$ where $L$ is an estimable contrast matrix. Details about how the DF is approximated can be found in the original paper. In a simpler case where $H_0 :l'\beta = 0$ and $r = 1,$ the Wald statistic is $$t = \frac{l'\hat\beta}{\sqrt{l\widehat{\text{Cov}}(\hat\beta)l'}},$$ which follows a $t$ distribution with unknown DF $\nu.$ @satt assume the quantity $$\frac{\nu(l'\widehat{\text{Cov}}(\hat\beta)l)}{l'\widehat{\text{Cov}}(\hat\beta)l},$$ and used a method-of-moments approach to obtain $\hat\nu = \frac{2(l'\widehat{\text{Cov}}(\hat\beta)l)^2}{\widehat{\text{Var}(l'\widehat{\text{Cov}}(\hat\beta)l)}},$ where the denominator is determined by a univariate delta method [@kuznetsova_lmertest_2017].

KR uses the same approach, but adjusts for the underestimation of $\text{Cov}(\hat\beta)$ in small sample sizes by including an adjusted estimator for the covariance matrix of $\hat\beta$ that inflates the covariance estimate in the numerator of the Wald statistic [@kuznetsova_lmertest_2017].

When $L=1$ the KR and Satterthwaite approximation will produce the same denominator degrees of freedom. However, since the statistic used for the two methods are not the same, the results for inference will not be the same, as KR adjusts the estimate for the covariance matrix of $\hat\beta.$

## Existing Literature

Both KR and Satterthwaite methods are frequently used and compared, and its performance is highly dependent on the structure of the data.
A majority of studies focusing on DF method comparison in mixed models use split-plot design, as small sample sizes are more common in agricultural and biological fields. @schaalje_adequacy_2002 found that in comparison to other degrees of freedom-adjusting methods like Satterthaite, KR was the most suitable for small sample data. Using factors such as imbalance, covariance structure, and sample size, they demonstrated that the KR method produced simulated Type I error rates closest to target values. However, their focus was primarily on complexity of covariance structure, and they found that more complicated structures, such as ante-dependence, produced inflated error rates when coupled with small sample size. @arnau_analyzing_2009 found that KR produces more robust results compared to Satterthwaite and Between-Within approaches, especially in cases where larger sample size was paired with covariance matricies with larger values. 

These studies are conducted with data drawn from normal distributions. However, real-world data used in fields such as psychometrics have distributions that are nonnormal. In their paper, @arnau_using_2012 extend their evaluation of KR for split-plot data that follow a log-normal or exponential distribution, and for when the kurtosis and skewness values are manipulated. They found that, compared to normal distribution, KR is less robust for log-normal distributions, but that there is no significant difference in performance between exponential and normal distributions. In addition, they suggest that skewness has a bigger effect on robustness of KR compared to kurtosis. 

Existing research evaluating the performance of methods that reduce Type I error rate in small samples are thorough, however, the differences in simulation setup and structure of data used make generalizations difficult. Although the KR method has been shown as a viable option for analysis of small samples in many occasions, it should continue to evaluated against other methods. To date, there is no literature on the performance of Satterthwaite for nonnormal longitudinal data design. Given the prevalence of nonnormal and small data samples, it is important to continue exploring methods that ensure robust results. 
