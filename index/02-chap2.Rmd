# R Markdown Basics {#rmd-basics}
<!--
This file is for including Chapter 2.  

Notice that it's also good practice to label your chapters and sections.  This will help you debug potential issues as you knit and allows you to link references throughout the document. Look for the reference to this chapter at the beginning of Chapter 3.

If labels aren't specified, they will automatically be generated from the header by changing the spaces to hyphens and capital letters to lowercase.  
-->
```{r load_packages2, include = FALSE}
library(mosaic)
```

Be careful with your spacing in _Markdown_ documents.  While whitespace largely is ignored, it does at times give _Markdown_ signals as to how to proceed.  As a habit, try to keep everything left aligned whenever possible, especially as you type a new paragraph.  In other words, there is no need to indent basic text in the Rmd document (in fact, it might cause your text to do funny things if you do).

## Lists

It's easy to create a list.  It can be unordered like

* Item 1
* Item 2

or it can be ordered like

1. Item 1
4. Item 2

Notice that I intentionally mislabeled Item 2 as number 4.  _Markdown_ automatically figures this out!  You can put any numbers in the list and it will create the list.  Check it out below.

To create a sublist, just indent the values a bit (at least four spaces or a tab).  (Here's one case where indentation is key!)

1. Item 1
1. Item 2
1. Item 3
    - Item 3a
    - Item 3b

#### Inference in small and nonnormal samples

In chapter 1, we outlined the process for conducting inference for models with repeated measures. When sample size is small, both Kenward-Rogers and Sattherthwaite approximations have been implemented to reduce Type I error rates. 

Kenward-Rogers (1997)(CITE) 










```{r cars}
summary(cars)
```

## Inline code

If you'd like to put the results of your analysis directly into your discussion, add inline code like this:

> The `cos` of $2 \pi$ is `r cos(2*pi)`. 

Another example would be the direct calculation of the standard deviation:

> The standard deviation of `speed` in `cars` is `r sd(cars$speed)`.

One last neat feature is the use of the `ifelse` conditional statement which can be used to output text depending on the result of an **R** calculation:

> `r ifelse(sd(cars$speed) < 6, "The standard deviation is less than 6.", "The standard deviation is equal to or greater than 6.")`

Note the use of `>` here, which signifies a quotation environment that will be indented.

As you see with `$2 \pi$` above, mathematics can be added by surrounding the mathematical text with dollar signs.  More examples of this are in [Mathematics and Science] if you uncomment the code in [Math].  

## Including plots

You can also embed plots.  For example, here is a way to use the base **R** graphics package to produce a plot using the built-in `pressure` dataset:

```{r pressure, echo = FALSE, cache = TRUE}
plot(pressure)
```

Note that the `echo=FALSE` parameter was added to the code chunk to prevent printing of the **R** code that generated the plot.  There are plenty of other ways to add chunk options.  More information is available at <http://yihui.name/knitr/options/>.  

Another useful chunk option is the setting of `cache=TRUE` as you see here.  If document rendering becomes time consuming due to long computations or plots that are expensive to generate you can use knitr caching to improve performance.  Later in this file, you'll see a way to reference plots created in **R** or external figures.

## Loading and exploring data

Included in this template is a file called `flights.csv`.  This file includes a subset of the larger dataset of information about all flights that departed from Seattle and Portland in 2014.  More information about this dataset and its **R** package is available at <http://github.com/ismayc/pnwflights14>.  This subset includes only Portland flights and only rows that were complete with no missing values.  Merges were also done with the `airports` and `airlines` data sets in the `pnwflights14` package to get more descriptive airport and airline names.

We can load in this data set using the following command:

```{r load_data}
flights <- read.file("data/flights.csv")
```

The data is now stored in the data frame called `flights` in **R**.  To get a better feel for the variables included in this dataset we can use a variety of functions.  Here we can see the dimensions (rows by columns) and also the names of the columns.


```{r str}
dim(flights)
names(flights)
```

```{r longstring, linewidth = 65, warnings = FALSE}
# read long paragraph file
longtext <- readLines("data/paragraphs.txt")
# display text as vector
longtext
# display text as paragraphs
cat(longtext)
```

```{r longstring2}
# display text without linewidth option specified
longtext
```


Another good idea is to take a look at the dataset in table form.  With this dataset having more than 50,000 rows, we won't explicitly show the results of the command here.  I recommend you enter the command into the Console **_after_** you have run the **R** chunks above to load the data into **R**.

```{r view_flights, eval=FALSE}
View(flights)
```

While not required, it is highly recommended you use the `dplyr` package to manipulate and summarize your data set as needed.  It uses a syntax that is easy to understand using chaining operations.  Below I've created a few examples of using `dplyr` to get information about the Portland flights in 2014.  You will also see the use of the `ggplot2` package, which produces beautiful, high-quality academic visuals.


The example we show here does the following:

- Selects only the `carrier_name` and `arr_delay` from the `flights` dataset and then assigns this subset to a new variable called `flights2`. 

- Using `flights2`, we determine the largest arrival delay for each of the carriers.


```{r max_delays}
flights2 <- flights %>% 
  select(carrier_name, arr_delay)
max_delays <- flights2 %>% 
  group_by(carrier_name) %>%
  summarize(max_arr_delay = max(arr_delay, na.rm = TRUE))
```


A useful function in the `knitr` package for making nice tables in _R Markdown_ is called `kable`.  It is much easier to use than manually entering values into a table by copying and pasting values into Excel or LaTeX.  This again goes to show how nice reproducible documents can be! (Note the use of `results="asis"`, which will produce the table instead of the code to create the table.)  The `caption.short` argument is used to include a shorter title to appear in the List of Tables.

```{r maxdelays, results="asis"}
kable(max_delays, 
      col.names = c("Airline", "Max Arrival Delay"),
      caption = "Maximum Delays by Airline",
      caption.short = "Max Delays by Airline",
      longtable = TRUE,
      booktabs = TRUE)
```

The last two options make the table a little easier-to-read.

We can further look into the properties of the largest value here for American Airlines Inc.  To do so, we can isolate the row corresponding to the arrival delay of 1539 minutes for American in our original `flights` dataset.


```{r max_props}
flights %>% filter(arr_delay == 1539, 
                  carrier_name == "American Airlines Inc.") %>%
  select(-c(month, day, carrier, dest_name, hour, 
            minute, carrier_name, arr_delay))
```

We see that the flight occurred on March 3rd and departed a little after 2 PM on its way to Dallas/Fort Worth.  Lastly, we show how we can visualize the arrival delay of all departing flights from Portland on March 3rd against time of departure.

```{r march3plot, fig.height=3, fig.width=6}
flights %>% filter(month == 3, day == 3) %>%
  ggplot(aes(x = dep_time, y = arr_delay)) + geom_point()
```


```{proof myproof, name = "This is a proof"}
There is a proof environment in which you can create equations

$$
\hat{\beta}_0 + \hat{\beta}_1x
$$
```


# Notes from lit review

Arnau (2014): Investigated robustness of the KR procedure when groups have different nonnormal distributions.Parameters considered: skewness, kurtosis, ratio of kurtosis differences across groups, unequal vs equal group sizes, pairing of large kurtosis with larger group size. violation of the sphericity assumption does not affect the robustness of the LMM combined with the KR procedure. the violation of skewness appears to have a greater effect on KR robustness than does the violation of kurtosis

 

KR was less robust when the relationship between the kurtosis values in the groups increased or when the largest group was associated with the largest value of kurtosis. Both of these effects on KR robustness were greater when total sample size was smaller, and especially when total sample sizes were 30.

he pairing of kurtosis with group size and the relationship between the kurtosis values in the groups are shown to be relevant variables to consider when using the LMM with KR

Arnau (2013): Specifically, we sought to examine whether
skewness and kurtosis have a differential effect on KR robustness by exploring both independently.that for the repeated measures effect
the LMM with KR was robust mainly when data were
normal, regardless of whether the sphericity assumption
was met. Likewise, for the interaction effect, the procedure
was also robust when the total sample size was 45 or larger,
but it was liberal when the total sample size was 30. 

arnau(2012): that the test is more robust with a normal than with a log-normal distribution, whereas, overall, there are no significant differ- ences in performance between normal and exponential dis- tributions. In addition, when the covariance matrix is spherical, the test tends to become more robust with normal and exponential distributions, especially when the number of observations increases; this is contrary to what occurs with the log-normal distribution. Finally, a comparison of the two exponential distributions shows that the test becomes more robust as kurtosis increases, regardless of whether or not the assumption of sphericity is fulfilled.

s compared to the estimation of the time effect alone, the interaction between time and group leads to a consider- able increase in the test’s robustness when the distribution is log-normal.

KR procedure is compromised with log- normal distributions that show moderate skewness, especial- ly as regards the estimation of the time effect. By contrast, when distributions are normal or have slight skewness (γ1 0 0.8), the test is robust even with extreme kurtosis 

vallejo: when data were obtained from a normal distribution, the
BF procedure of the main effect provided better control of the Type I error rate than
did the Proc Mixed solution based on either the AIC or BIC criterion. For the non-normal distributions, both approaches become conservative or liberal
as a function of the covariance structure and forms of the distribution used to generate the data. Nonetheless, the BF test was slightly less liberal than the test based
on either the AIC or BIC criterion when both procedures had Type I error rates
above the significance level and slightly more conservative when both had Type I
error rates below the significance level. With regard to power, no test was uniformly most powerful



## Additional resources

- _Markdown_ Cheatsheet - <https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet>

- _R Markdown_ Reference Guide - <https://www.rstudio.com/wp-content/uploads/2015/03/rmarkdown-reference.pdf>

- Introduction to `dplyr` - <https://cran.rstudio.com/web/packages/dplyr/vignettes/introduction.html>

- `ggplot2` Documentation - <http://docs.ggplot2.org/current/>
