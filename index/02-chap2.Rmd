---
output:
  pdf_document: default
  html_document: default
---
# Methods for tests of fixed effects in small and nonnormal samples {#rmd-basics}
<!--
This file is for including Chapter 2.  

Notice that it's also good practice to label your chapters and sections.  This will help you debug potential issues as you knit and allows you to link references throughout the document. Look for the reference to this chapter at the beginning of Chapter 3.

If labels aren't specified, they will automatically be generated from the header by changing the spaces to hyphens and capital letters to lowercase.  
-->
```{r load_packages2, include = FALSE}
library(mosaic)
library(kableExtra)
library(ggplot2)
library(SimMultiCorrData)
library(here)
library(tidyverse)
library(kableExtra)
knitr::opts_chunk$set(echo = FALSE, message = F, warning = F)
```

In chapter 1, we outlined the basics of analyzing longitudinal data and introduced linear mixed models. Next, we will examine inference of linear mixed models, and how methods such as Kenward-Roger (KR) and Satterthwaite can be used in situations where standard procedures for inference may produce questionable results. 

## Inference 
In statistical inference, the goal is to make conclusions about the underlying characteristics of a set of data and establish a relationship between certain variables. Hypothesis testing is one of the primary examples of inference, and is carried out in order to assess the true value of a population parameter. In linear models, the significance of a slope parameter, $\beta_k$, is often assessed, where the null hypothesis, $H_0 $ is $\beta_k = 0,$ and the alternative hypothesis $H_a$ is $\beta_k \neq 0.$ A test of the null hypothesis involves using a Wald statistic in the form $$ Z = \frac{\hat\beta_k}{\sqrt{\hat{Var(\hat\beta_k)}}},$$ which is then compared to the normal distribution, and a subsequent p-value is obtained. 

Building on foundations of a general linear hypothesis test, given a matrix $L$ of size $q \times p,$ where $q$ represents the number of estimable functions of $\beta,$ $$(L\hat\beta-L\beta)'[L(X'\widehat {Cov}(\hat\beta)X)^-L']^{-1}(L\hat\beta-L\beta)
$$ is approximately $\chi^2(q)$ (Rencher and Schaalje, 2008). For a null hypothesis $H_0: L\beta = 0,$ the test statistic $G$ is $$(L\hat\beta)'[L(X'\widehat {Cov}(\hat\beta)X)^-L']^{-1}(L\hat\beta).$$ 

Aside from using the Wald statistic, likelihood ratio tests are another method to make inferences about $\beta$, and involves comparing two models: (1) a nested model, which assumes that $\beta_k$ is 0, and (2) a full model, that allows $\beta_k$ to vary without constraint. The difference in the maximized log-likelihood of the two models, $\hat l_{reduced}$ and $\hat l_{full}$ are compared. This difference is represented by the statistic $$G^2 = 2(\hat l_{full}-\hat l_{reduced}),$$ which is compared to a chi-square distribution. The larger the difference, the more likely we are to conclude that the nested model is insufficient, and that $\beta$ is not zero. 
While there are benefits to using the likelihood ratio test, the rest of this study will focus on method of using the Wald statistic. 

### Inference in small sample sizes 
One crucial assumption when conducting inference using the ML estimate for $\beta$ is that the sample size is sufficient enough where it does not affect the estimate for $\Sigma_i.$ However, what happens when the sample size is too small? This causes $\hat\Sigma_i$ to underestimate the true variance, which in turn causes $\widehat {\text{Cov}}(\hat\beta)$ to be too small since it relies on covariance estimator. If $\widehat {\text{Cov}}(\hat\beta)$ is too small, the denominator of the test statistic is inflated, leading to increased Type I error. One can see that the bias of the covariance estimator weakens the entire foundation of estimation and inference. 

In very limited cases, where data are complete, balanced, and produce nonnegative values in REML estimation, is it possible to perform exact small-sample inferences. If $[L(X'\widehat {\text{Cov}}(\hat\beta)X)^-L']^{-1}$ with $g$ degrees of freedom can be rewritten such that $$ \frac{(L\hat\beta)'Q(L\hat\beta)}{g}\frac{w}{d} =  \frac{(L\hat\beta)'[L(X'\widehat {\text{Cov}}(\hat\beta)X)^-L']^{-1}(L\hat\beta)}{g},$$ where $w$ is a chi-square random variable with $d$ degrees of freedom. If so, this statistic is F-distributed. 

However, in most scenarios, an approximate small-sample method must be used, in which the statistic $$F = \frac{(L\hat\beta)'[L(X'\widehat {\text{Cov}}(\hat\beta)X)^-L']^{-1}(L\hat\beta)}{g}$$ follows a distribution with numerator degrees of freedom $g,$ and unknown denominator degrees of freedom (DDF). There are several ways to approximate the DDF. 


Both Satterthwaite and KR are proposed methods of reductions to the DDF when conducting tests in order to account for the uncertainty of the covariance estimator. The KR method goes one step forward to also adjust the test statistic itself.


## Satterthwaite 

Sattherthwaite approximation was developed by Fai & Cornelius (1996), with the F statistic following the form:

$$F = \frac{1}{l}\hat\beta'L'(L\widehat {\text{Cov}}(\hat\beta) L')^{-1}L\hat\beta.$$ For the denominator degrees of freedom we perform spectral decomposition on $L'\widehat {\text{Cov}}(\hat\beta) L=P'DP,$ where $D$ is a diagonal matrix of eigenvalues and $P$ is an orthogonal matrix of eigenvectors. When $r$ represents the $r^{th}$ row of $P'L$, we have $v_r = \frac{2(d_r)^2}{g'_rWg_r},$ where $g_r$ is a gradient vector, $d_r$ is the $r^{th}$ diagonal element of D, and $W$ is the covariance matrix of $\hat\sigma^2.$ The denominator degrees of freedom is calculated by:
$$\frac{2E}{E-l},$$ where $E = \sum_{r = 1}^{l} \frac{v_r}{v_r-2}I(v_r>2)$ if $E >l$, otherwise $DF = 1.$

When $l =1$ the KR and Satterthwaite approximation will produce the same denominator degrees of freedom. However, since the statistic used for the two methods are not the same, the results for inference will not be the same. It is important to note that both methods are only valid when using REML. 

## Kenward-Roger

In Kenward-Roger (1997), a Wald statistic is proposed in the form of:
$$F = 1/l(\hat\beta-\beta)^TL(L^T\hat\Phi_A L)^{-1}L^T(\hat\beta-\beta),$$ where $l$ represents the number of linear combinations of the elements in $\beta,$ $L$ is a fixed matrix, and $\hat\Phi_A$ is the adjusted estimator for the covariance matrix of $\hat\beta$. As mentioned previously, $\widehat {\text{Cov}}(\hat\beta)$ is a biased estimator of $\text{Cov}(\hat\beta)$ when samples are small, and underestimates. This adjusted estimator is broken down into $\hat\Phi_A = \widehat {\text {Cov}}(\hat\beta) + 2\hat\Lambda$, where $\hat\Lambda$ accounts for the amount of variation that was underestimated by the original estimator of covariance of $\hat\beta$. The value $\Lambda$ is approximated using a Taylor series expansion around $\sigma,$ to be $$\Lambda\text{Cov}(\hat\beta)[\sum_{i=1}^{r}\sum_{j=1}^{r}W_{ij}(Q_{ij}-P_i\text{Cov}(\hat\beta)P_j)]\text{Cov}(\hat\beta),$$ where
$P_i = X^T\frac{\partial\Sigma^{-1}}{\partial\sigma_i}X$
$Q_{ij} = X^T \frac{\partial\Sigma^{-1}}{\partial\sigma_i}\Sigma\frac{\partial\Sigma^{-1}}{\partial\sigma_j}X,$ and $W_{ij}$ is the $(i,j)$th element of $\text W = V[\hat\sigma].$

This Wald statistic that uses the adjusted estimator is scaled in the form: $$F^* = \frac{m}{m+l-1}\lambda F,$$ where $m$ is the denominator degrees of freedom, and $\lambda$ is a scale factor. Using the expectation and variance of the Wald statistic, $F$ Both $m$ and $\lambda$ need to be calculated from the data, such that:

$$m = 4 + \frac{l+2}{l\rho-1},$$ where $\rho = \frac{V[F]}{2E[F]^2}$ and 
$\lambda = \frac{m}{E[F](m-2)}.$ This statistic will ultimately follow an exact $F_{l,m}$ distribution. 

## Other methods

*Residual DDF:* The DDF is calculated as $N-rank[X],$ where N is the total number of individuals in the dataset. This method is only suitable for data that are independent and identically distributed, so it is not typically used in linear mixed models.

*Containment Method* 
In the containment method, random effects that contain the fixed effect of interest are isolated. The smallest rank contribution to the $[X Z]$ matrix among these random effects becomes the DDF. If there are no effects found, then the DDF is equal to the residual DDF. 

*Between-Within Method* 
Schluchter and Elashoff (1990) propose a DDF method where residual DDF are calculated for both between-subject and within-subject subgroups. If there are changes in the fixed effect within subjects, then the within-subject DDF is used, otherwise the between-subject DDF is used. 

## Existing literature

Both KR and Satterthwaite methods are frequently used and compared, and its performance is highly dependent on the structure of the data.
A majority of studies focusing on DF method comparison in mixed models use split-plot design, as small sample sizes are more common in agricultural and biological fields. Schaalje, et al. (2002) found that in comparison to other degrees of freedom-adjusting methods like Satterthaite, KR was the most suitable for small sample data. Using factors such as imbalance, covariance structure, and sample size, they demonstrated that the KR method produced simulated Type I error rates closest to target values. However, their focus was primarily on complexity of covariance structure, and they found that more complicated structures, such as ante-dependence, produced inflated error rates when coupled with small sample size. Arnau (2009) found that KR produces more robust results compared to Satterthwaite and Between-Within approaches, especially in cases where larger sample size was paired with covariance matricies with larger values. 

These studies are conducted with data drawn from normal distributions. However, real-world data used in fields such as psychometrics have distributions that are nonnormal. In Arnau et. al's 2012 paper, the authors extend their evaluation of KR for split-plot data that follow a log-normal or exponential distribution, and for when the kurtosis and skewness values are manipulated. They found that, compared to normal distribution, the test is less robust for log-normal distributions, but that there is no signficant difference in performance between exponential and normal distributions. In addition, they suggest that skewness has a bigger effect on robustness of KR compared to kurtosis. 

Existing research evaluating the performance of methods that reduce Type I error rate in small samples are thorough, however, the differences in simulation setup and structure of data used make generalizations difficult. Although the KR method has been shown as a viable option for analysis of small samples in many occasions, it should continue to evaluated against other methods. To date, there is no literature on the performance of Satterthwaite for nonnormal longitudinal data design. Given the prevalence of nonnormal and small data samples, it is important to continue exploring methods that ensure robust results. 


## Goals of this study:

In this study, we aim to expand on previous simulations, evaluating how methods for evaluated fixed effects perform under different nonnormal distributions and sample sizes. The aforementioned studies often use a split-plot design and impose a covariance structure, but goal of this study will be to compare performance of KR and Satterthwaite methods for repeated measures longitudinal data fitted with a linear mixed effects model, and no imposed covariance structure. Since most mixed models use unstructured covariance structure, it would be beneficial to see how these methods perform without considering covariance structure as a factor.



## Simulation Set up:

### Generating data: Sample size

In this study, we consider a linear mixed effects model with two discrete covariates: time and treatment. The range of possible values that time takes on depends on how many number of measurements per individual, which can be 4 or 8. The treatment covariate takes on values of 0 or 1, and each assigned to half of the sample. The number of individuals take on possible values of 10, 18, and 26. These were chosen to reflect possible samples that would not hold under the common assumption that the sample size must be at least 30 for it to be considered sufficient enough for the Central Limit Theorem to hold. 

### Generating data: Fixed Effects

We have three fixed effects: the intercept value and the covariates time and treatment. The intercept, an arbitrary value, is set at 3.1. Time and treatment have a value of 0, and the Type I error rates of treatment will be evaluated. 

### Generating data: Random effects

In order to generate a continuous response variable that is nonnormal, we generate our random effects values from nonnormal distributions, which are either exponential or lognormal. Previous research shows that many data used in social and health sciences follow nonnormal distributions (Limpert, Stahel, & Abbt, 2001). More specifically  many follow lognormal distributions, such as age of onset of Alzheimer’s disease (Horner, 1987), or exponential distribution to model survival data. In order to cover a wide range of exponential and lognormal distributions, parameters were chosen to model distinct distributions. For exponential distributions, lambda values of .2, and .9 were used, (DO I NEED TO INSERT GRAPH?). For lognormal distribution, mean and standard deviation parameter combinations were (0,.25), and (1,.5). 

Using the `SimMultiCorrData` package, we derive kurtosis and skewness values based on the distributions specified above. The table below shows the range of skewness and kurtosis values for the Lognormal distribution. In the intercept only model, only one non-normal continuous variable is generated for the random effect, so the function `SimMultiCorrData::nonnormvar1` is used. Values are generated through Fleishman's method for simulating nonnormal data by matching moments using mean, variance, skew, and kurtosis and then transforming normally distributed values. 

Kurtosis and skew values for the distributions used in this simulation are shown below. 

```{r, echo = F}
rbind(
  calc_theory(Dist = "Lognormal", params = c(0, .25)),
  calc_theory(Dist = "Lognormal", params = c(1, .5)),
  calc_theory(Dist = "Exponential", params = .2),
  calc_theory(Dist = "Exponential", params = .9)
) %>%
  kable()
```


In the case of the linear model that has both random effects for intercept and slope, we want to generate random effects values that are correlated. Using `SimMultiCorrData::rcorrvar`, we use a similar process for generating one nonnormal continuous variable, but extend it to generating variables from multivariate normal distribution that take in to account a specified correlation matrix, and are then transformed to be nonnormal. We use a correlation value of -.38 to generate the random effects, which is based off the correlation observed when fitting a linear mixed effects model from the dataset used in the application portion of this study. 

***** FIX THIS ***** Lastly, to account for measurement/sampling error, we assume that the error is random and drawn from a $N \sim (0,.2).$ The standard deviation value was chosen to minimize the variation of the errors in relation to the random effects of the intercept and the covariate. 



### Linear mixed effects model

In a linear mixed effects model, the amount of random effects that will be modeled depends on the research question at hand. Here, we will examine both a random intercepts-only model, where the intercept of the model is assumed to have a random effects structure, as well as a random intercept and slope model, where in addition to intercept, the covariate time will also have a random effects structure. 

We use the `lmerTest` package to fit the linear mixed effects model, and evaluate the significance of the covariate in the model. To evaluate significance, we compare both the KR and Satterthwaite method for adjusting denominator degrees of freedom and its resulting p-value.   Because the value of the covariate in our model is fixed at 0 in order to identify Type I error, we expect to see that the p-value for the covariate time to not be significant ($p$ > .05) in an ideal scenario. 


## Evaluating and Results

```{r, echo=FALSE}
options(scipen = 9999)
all_sim <- readRDS(here("SimulationData", "all_sim.rds"))

all_sim_sum <- all_sim %>%
  group_by(
    distribution,
    number_individuals,
    params,
    number_measurements,
    rslope,
    KR_effect, KR_term, skew, kurtosis
  ) %>%
  summarize(
    KR_t1err = mean(KR_sig),
    S_t1err = mean(S_sig),
    Z_t1err = mean(Z_sig),
    t_t1err = mean(t_sig)
  )
```


After performing 400 replications of each condition at a significance level of .05, we evaluate robustness using Bradley's criterion,which considers a test to robust if the empirical error rate is between .025 and .075. In the following section, we will compare Type I error rates produced from KR and Satterthwaite methods as well as t-as-z and using the standard DF formula, further stratified by distribution and other manipulated parameters. T-as-z and standard DF formula are not adjustments to account for smaller sample sizes, and are used as  comparison to Satterthwaite and KR, since they are expected to be anti-conservative. 


```{r}
transformed_data <- all_sim_sum %>%
  filter(KR_term == "treatment") %>%
  pivot_longer(cols = c("KR_t1err", "S_t1err", "Z_t1err", "t_t1err"), names_to = "DF_method", values_to = "error_rate") %>%
  mutate(type = paste(distribution, params, sep = ":"))

ggplot(aes(x = as.factor(number_individuals), y = error_rate, color = DF_method,
           group = as.factor(number_measurements)), data = transformed_data) +
  facet_grid(rslope ~ type) +
  geom_point(alpha = .5, aes(shape = as.factor(number_measurements)), position = position_jitter(h = 0, w = 0.3)) +
  geom_hline(yintercept = .05) +
  scale_y_continuous(limits = c(0, .25)) +
  annotate("rect", xmin = -.1, xmax = 4, ymin = 0.025, ymax = .075, alpha = .4) +
  xlab("Number of Individuals") +
  ylab("Error Rate") +
  labs(color = "DF Method", group = "Number of Measurements")
```
FIGURE 1 displays error rates from all 4 degrees of freedom methods by distribution, parameters, complexity of random model, number of measurements, and number of samples. The shaded region indicates error rates that are considered robust by Bradley's criterion. It is evident that there are varying patterns of performance by distribution. The common conception that larger sample sizes or large number of measurements can improve robustness is not necessarily evident across all distributions, for example in the case of the exponential distribution. One trend that appears to be evident across all three distributions is that when the degrees of freedom methods are applied to a random intercept model, a more structurally simple model, they yield more robust error rates in comparison to an application to the random slopes model. 

In addition, when looking at performance of the 4 methods overall, we can see that the t-as-z and standard DF approach produce significantly more anti-conservative results, regardless of the values of other parameters. These trends align closely with a previous study by @luke_evaluating_2017 examining only normal distributions. 

In order to make more specific observations and identify trends, we will examine performance within each of the three distributions by sample size and number of measurements.  

### Exponential Distribution

```{r}

ggplot(aes(x = as.factor(number_individuals), y = error_rate, color = DF_method, 
           group = as.factor(number_measurements)), data = transformed_data %>% filter(distribution == "Exponential")) +
  facet_grid(rslope ~ type) +
  geom_point(alpha = .5, aes(shape = as.factor(number_measurements)), position = position_jitter(h = 0, w = 0.3)) +
  geom_hline(yintercept = .05) +
  scale_y_continuous(limits = c(0, .25)) +
  annotate("rect", xmin = -.1, xmax = 4, ymin = 0.025, ymax = .075, 
           alpha = .4) +
  xlab("Number of Individuals") +
  ylab("Error Rate") +
  labs(color = "DF Method", group = "Number of Measurements")

transformed_data %>%
  filter(distribution == "Lognormal") %>%
  mutate(robust = ifelse(error_rate >= .025 & error_rate <= .075, TRUE, 
                         FALSE)) %>%
  group_by(type) %>%
  summarize(robustness = mean(robust))
```
Our simulation results contain two exponential distribution, one with $\lambda = .9$ and $\lambda = .2$.  At $\lambda = .2$, we can see that in random slope models, increasing the sample size from 10 to 26 only marginally improves the DF methods, and more specifically, on those that are applied to conditions with more repeated measurements. For random slope models that have samples smaller than 26, all methods do not differ too much in terms of performance, and tend to be anti-conservative. On the other hand, in random intercept models, increasing the sample size did not improve the performance of DF methods, and in some cases caused the DF methods to be more anti-conservative; in the case of sample size 26 and 4 measurements, Type I error rates performed significantly worse in comparison to smaller sample sizes, holding other conditions constant. 

At $\lambda = .9$, we see virtually the same trends in terms of the effect of sample size, complexity of model, and number of measurements on the performance of the DF methods. Overall, across both distributions, increasing the number of repeated measures impacted the DF methods' performance and increased robustness, while increasing sample size had less of an effect. Additionally, Kenward-Roger and Satterthwaite methods tended to produce more conservative error rates.  

Despite having different parameter values, the application of DF methods to these two exponential distributions produce similar different trends in error rates. Considering that the two distributions have the same skewness and kurtosis values, we hypothesize whether this similarity in values contributes to the performance of the DF methods. Next, we will examine the performance of DF methods in lognormal distributions. 


## Lognormal 
```{r}
ggplot(aes(x = as.factor(number_individuals), y = error_rate, color = DF_method, 
           group = as.factor(number_measurements)), data = transformed_data %>% filter(distribution == "Lognormal")) +
  facet_grid(rslope ~ type) +
  geom_point(alpha = .5, aes(shape = as.factor(number_measurements)), position = position_jitter(h = 0, w = 0.3)) +
  geom_hline(yintercept = .05) +
  scale_y_continuous(limits = c(0, .25)) +
  annotate("rect", xmin = -.1, xmax = 4, ymin = 0.025, ymax = .075, 
           alpha = .4) +
  xlab("Number of Individuals") +
  ylab("Error Rate") +
  labs(color = "DF Method", group = "Number of Measurements")
```
As seen in the first figure, across the lognormal distributions, DF methods applied to random intercept models had consistently more robust error rates in comparison to random slope. 

Our first lognormal distribution with parameters $(0,.25)$ has lower values of kurtosis and skewness. In contrast to the exponential distributions, DF methods appear to be positively impacted by increasing the sample size; this can be seen in both random intercept and random slope model, and the effect is compounded when there are 8 measurements as opposed to 4. In the random slopes model, one can see that the error rates produced by the T-as-z and standard DF method are reduced by one fourth when increasing the sample size from 10 to 26. 

On the other hand, with higher levels of skewness and kurtosis with a lognormal distribution with parameters $(1,.5)$, the effect of number of measurements and sample size are slightly different. While increasing the sample size impacts the DF methods in random intercept models regardless of the number of measurements, it seems to have less of an impact in random slope models when the number of measurements is 4 rather than 8. Overall, it appears that the number of measurements has a significant effect on the performance of the DF methods, as DF methods that are applied to models with larger measurements consistently are more robust and less anti-conservative. In addition, DF methods that are applied to this distribution are slightly less robust on average in comparison to the first distribution. 

Based on comparisons between the two distributions, our results suggest that increasing skewness and kurtosis can negatively impact robustness of DF methods, and that it can also affect the impact of sample size on how the DF methods will perform. 

## Normal Distribution 
```{r}
ggplot(aes(x = as.factor(number_individuals), y = error_rate, color = DF_method, 
           group = as.factor(number_measurements)), data = transformed_data %>% filter(distribution == "Gaussian")) +
  facet_grid(rslope ~ type) +
  geom_point(alpha = .5, aes(shape = as.factor(number_measurements)), position = position_jitter(h = 0, w = 0.3)) +
  geom_hline(yintercept = .05) +
  scale_y_continuous(limits = c(0, .30)) +
  annotate("rect", xmin = -.1, xmax = 4, ymin = 0.025, ymax = .075, 
           alpha = .4) +
  xlab("Number of Individuals") +
  ylab("Error Rate") +
  labs(color = "DF Method", group = "Number of Measurements")
```
While nonnormal distributions are the focus of this study, comparing performance of DF methods to the normal distribution is important as a point of reference. It is interesting to note how robustness has decreased for DF methods applied to the normal distribution compared to lognormal and exponential distributions, but it is also important to acknowledge that the methods, especially KR and Satterthwaite, produce conservative error rates, rather than anti-conservative ones. In general, in both random slope and random intercept models, increasing the sample size seems to decrease anti-conservative error rates and increase conservative error rates produced by DF methods and bring them closer to robustness. T-as-z and standard DF are more likely to be anti-conservative, while KR and Satterthwaite are more conservative. Increasing the number of measurements isn't strongly associated with DF methods producing more robustness results, but it is associated with more conservative error rates, which is arguably more ideal. 


### KR vs Satterthwaite 

Comparing performance across all 4 methods has yielded signficant evidence that KR and Satterthwaite are superior methods when using linear mixed models on small samples. @luke_evaluating_2017 suggests that both KR and Satterthwaite are comparable solutions to obtain adequate Type I error. The following figure  aims to narrow in on differences in performance between the two methods. One can see that across random intercept models, KR and Satterthwaite methods have identical performance. Looking more closely at random slope models, it appears that KR consistently produces more conservative error rates when holding distribution, sample size, and number of measurements constant. This effect is further amplified when sample sizes are small. Overall, we find that KR is the more optimal DF method across nonnormal and normal distributions.
```{r}
all_sim_sum %>%
  filter(KR_term == "treatment") %>%
  pivot_longer(cols = c("KR_t1err", "S_t1err", "Z_t1err", "t_t1err"), names_to = "DF_method", values_to = "error_rate") %>%
  filter(DF_method %in% c("KR_t1err", "S_t1err")) %>%
  pivot_wider(
    names_from = c("rslope"),
    values_from = "error_rate", names_prefix = "slope"
  ) %>%
  group_by(number_individuals, skew, kurtosis, DF_method) %>%
  summarize(random_intercept = mean(slopeFALSE), 
            random_slope = mean(slopeTRUE)) %>%
  pivot_wider(
    names_from = c("number_individuals"),
    values_from = c("random_intercept", "random_slope")
  ) %>%
  select(
    skew, kurtosis, DF_method, random_intercept_10, random_slope_10, random_intercept_18,
    random_slope_18, random_intercept_26, random_slope_26
  ) %>%
  ungroup() %>%
  kbl() %>%
  pack_rows("Exponential", 7, 8) %>%
  pack_rows("Normal", 1, 2) %>%
  pack_rows("Lognormal", 3, 6) %>%
  add_header_above(c(
    " " = 3, "Random Intercept" = 1, "Random Slope" = 1, "Random Intercept" = 1,
    "Random Slope" = 1, "Random Intercept" = 1, "Random Slope" = 1
  )) %>%
  add_header_above(c(" " = 3, "10" = 2, "18" = 2, "26" = 2)) %>%
  add_header_above(c(" " = 3, "Sample Size" = 6)) %>%
  landscape()


ggplot(aes(x = as.factor(number_individuals), y = error_rate, 
           color = DF_method, group = as.factor(number_measurements)), data = transformed_data %>% filter(DF_method %in% c("KR_t1err", "S_t1err"))) +
  facet_grid(rslope ~ type) +
  geom_point(alpha = .5, aes(shape = as.factor(number_measurements)), position = position_jitter(h = 0, w = 0.1)) +
  geom_hline(yintercept = .05)+
  annotate("rect", xmin = -.1, xmax = 4, ymin = 0.025, ymax = .075, alpha = .4)
```

### KR Only

While KR method appears to be the most robust adjustment, (TABLE 5?) depicts its relatively variable performance across different conditions. Careful consideration must be used when conducting inference, and if possible, an increase in both sample size and number of measurements appears to ensure more robust results. 
```{r}
all_sim_sum %>%
  filter(KR_term == "treatment") %>%
  pivot_wider(
    names_from = c("number_individuals", "rslope"),
    values_from = "KR_t1err"
  ) %>%
  group_by(distribution, params, number_measurements) %>%
  summarise_all(funs(.[!is.na(.)])) %>%
  slice(1) %>%
  ungroup() %>%
  select(-c(KR_effect, KR_term, S_t1err, Z_t1err, t_t1err, distribution)) %>%
  kbl() %>%
  pack_rows("Exponential", 1, 4) %>%
  pack_rows("Normal", 5, 6) %>%
  pack_rows("Lognormal", 7, 10) %>%
  add_header_above(c(
    " " = 4, "Random Intercept" = 1, "Random Slope" = 1, "Random Intercept" = 1,
    "Random Slope" = 1, "Random Intercept" = 1, "Random Slope" = 1
  )) %>%
  add_header_above(c(" " = 4, "10" = 2, "18" = 2, "26" = 2)) %>%
  add_header_above(c(" " = 2, "Sample Size" = 8)) %>%
  landscape()
```

## Discussion

Overall trends:

increasing sample sizes matters less when skewness and kurtosis are larger! 
increasing number of measurements is always optimal 
In normal distributions KR and Satterthwaite seem to be too conservative, but they work well in nonnormal distributions. 


Ultimately, these results strongly support using either KR or Satterthwaite degrees of freedom adjustments as opposed to methods aimed towards larger sample sizes. 
